{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Multimodal.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 数据读取与处理"
   ],
   "metadata": {
    "id": "eCTbyIEz9a0z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers"
   ],
   "metadata": {
    "id": "dreqyj1C4iGI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "id": "sqT6fILb_VSr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXPOmIHXvgFD",
    "outputId": "5b39eaf4-4915-4e10-9487-2e6bbb251a24",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Multimodal\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "%cd drive/MyDrive/Multimodal/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "读取数据，记录训练集与测试集的id，将label转化成0、1、2三类，对应积极、中立、消极三种情感"
   ],
   "metadata": {
    "id": "jHft6TmgYf5_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open('./train.txt', 'r') as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for line in lines[1:]:\n",
    "  data = {}\n",
    "  line = line.replace('\\n','')\n",
    "  guid, tag = line.split(',')\n",
    "  if tag == 'positive':\n",
    "    label = 0\n",
    "  elif tag == 'neutral':\n",
    "    label = 1\n",
    "  else:\n",
    "    label = 2\n",
    "  data['guid'] = guid\n",
    "  data['label'] = label\n",
    "  train_set.append(data)\n",
    "\n",
    "# print(len(train_set)) # 4000\n",
    "# print(train_set)"
   ],
   "metadata": {
    "id": "3UPI9L_IYfd2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('./test_without_label.txt', 'r') as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "test_set = []\n",
    "for line in lines[1:]:\n",
    "  data = {}\n",
    "  data['guid'] = line.split(',')[0]\n",
    "  test_set.append(data)"
   ],
   "metadata": {
    "id": "GmZ2Ead7onSo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "对所有图像的大小进行统一，规范至(224, 224, 3)，符合ResNet18的输入大小\n",
    "\n",
    "读取并存储文本至数据集中，无法解码的字符使用ignore进行忽略"
   ],
   "metadata": {
    "id": "DORiEpFlMAmM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def data_process(dataset):\n",
    "  for data in dataset:\n",
    "    guid = data['guid']\n",
    "    image_path = './data/' + guid + '.jpg'\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    array = np.array(image.resize((224, 224)))\n",
    "    data['image'] = array.reshape((3, 224, 224))\n",
    "\n",
    "    text_path = './data/' + guid + '.txt'\n",
    "    f = open(text_path, 'r', errors='ignore')\n",
    "    lines = f.readlines()\n",
    "    # print(lines)\n",
    "    text = ''\n",
    "    for line in lines:\n",
    "      text += line\n",
    "    data['text'] = text"
   ],
   "metadata": {
    "id": "XnGuCctkANHL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_process(train_set)\n",
    "data_process(test_set)"
   ],
   "metadata": {
    "id": "IANjDjb8vQqX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "划分数据集，验证集采用和测试集相近的数目（500条）"
   ],
   "metadata": {
    "id": "bJE2uZYMighe",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_num = 3500\n",
    "valid_set_num = 500\n",
    "train_set, valid_set = random_split(train_set, [train_set_num, valid_set_num])"
   ],
   "metadata": {
    "id": "2V8Jng18ixm2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 图像分类器\n",
    "\n",
    "图像分类采用的ResNet18模型，在第三次实验中表现最佳\n",
    "\n",
    "定义残差块ResBlock和ShorcutResBlock，前者不改变通道数，后者会改变通道数："
   ],
   "metadata": {
    "id": "0UL7K4foLEA0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, input_channel, output_channel):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(input_channel, output_channel, kernel_size=(3, 3), padding=1, stride=1)\n",
    "    self.bn1 = nn.BatchNorm2d(output_channel)\n",
    "    self.conv2 = nn.Conv2d(output_channel, output_channel, kernel_size=(3, 3), padding=1, stride=1)\n",
    "    self.bn2 = nn.BatchNorm2d(output_channel)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    output = self.conv1(x)\n",
    "    output = self.bn1(output)\n",
    "    output = F.relu(output)\n",
    "    output = self.conv2(x)\n",
    "    output = self.bn2(output)\n",
    "    output = F.relu(output + x)\n",
    "    return output\n",
    "\n",
    "\n",
    "class ShortcutResBlock(nn.Module):\n",
    "  def __init__(self, input_channel, output_channel):\n",
    "    super(ShortcutResBlock, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(input_channel, output_channel, kernel_size=(1, 1), stride=2)\n",
    "    self.bn1 = nn.BatchNorm2d(output_channel)\n",
    "    self.conv2 = nn.Conv2d(input_channel, output_channel, kernel_size=(3, 3), padding=1, stride=2)\n",
    "    self.bn2 = nn.BatchNorm2d(output_channel)\n",
    "    self.conv3 = nn.Conv2d(output_channel, output_channel, kernel_size=(3, 3), padding=1, stride=1)\n",
    "    self.bn3 = nn.BatchNorm2d(output_channel)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output1 = self.conv1(x)\n",
    "    output1 = self.bn1(output1)\n",
    "    output2 = self.conv2(x)\n",
    "    output2 = self.bn2(output2)\n",
    "    output2 = F.relu(output2)\n",
    "    output2 = self.conv3(output2)\n",
    "    output2 = self.bn3(output2)\n",
    "    output = F.relu(output1 + output2)\n",
    "    return output"
   ],
   "metadata": {
    "id": "Vm71E4uBG14h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义完整的ResNet18："
   ],
   "metadata": {
    "id": "9DocqDfm7M15",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ResNet18(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ResNet18, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), padding=3, stride=2)\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=(3, 3), padding=1, stride=2)\n",
    "    self.res1 = ResBlock(64, 64)\n",
    "    self.res2 = ResBlock(64, 64)\n",
    "    self.shortcut1 = ShortcutResBlock(64, 128)\n",
    "    self.res3 = ResBlock(128, 128)\n",
    "    self.shortcut2 = ShortcutResBlock(128, 256)\n",
    "    self.res4 = ResBlock(256, 256)\n",
    "    self.shortcut3 = ShortcutResBlock(256, 512)\n",
    "    self.res5 = ResBlock(512, 512)\n",
    "    self.pool2 = nn.AvgPool2d((7, 7))\n",
    "    self.dropout = nn.Dropout(0)\n",
    "    self.fc = nn.Linear(512, 3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.conv1(x)\n",
    "    output = self.bn1(output)\n",
    "    output = self.pool1(F.relu(output))\n",
    "    output = self.res1(output)\n",
    "    output = self.res2(output)\n",
    "    output = self.shortcut1(output)\n",
    "    output = self.res3(output)\n",
    "    output = self.shortcut2(output)\n",
    "    output = self.res4(output)\n",
    "    output = self.shortcut3(output)\n",
    "    output = self.res5(output)\n",
    "    output = self.pool2(output)\n",
    "    output = torch.flatten(output, 1)\n",
    "    output = self.fc(output)\n",
    "    return output"
   ],
   "metadata": {
    "id": "L4ITJsRq6ujZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "验证一下图像分类器单独分类的性能\n",
    "\n",
    "使用TensorDataset生成训练图像分类器的数据集："
   ],
   "metadata": {
    "id": "O2pRSAHUoQMn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image_train = []\n",
    "image_train_labels = []\n",
    "image_valid = []\n",
    "image_valid_labels = []\n",
    "\n",
    "for data in train_set:\n",
    "  image_train.append(data['image'])\n",
    "  image_train_labels.append(data['label'])\n",
    "\n",
    "for data in valid_set:\n",
    "  image_valid.append(data['image'])\n",
    "  image_valid_labels.append(data['label'])\n",
    "\n",
    "image_train = torch.from_numpy(np.array(image_train))\n",
    "image_train_labels = torch.from_numpy(np.array(image_train_labels))\n",
    "image_valid = torch.from_numpy(np.array(image_valid))\n",
    "image_valid_labels = torch.from_numpy(np.array(image_valid_labels))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(image_train, image_train_labels), batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(image_valid, image_valid_labels), batch_size=50)"
   ],
   "metadata": {
    "id": "xxIAdJOmd0JA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练图像分类器，学习率5e-6，训练50个epoch"
   ],
   "metadata": {
    "id": "0QsBp-1toaI1",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "iSAd-Sdkn_jW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "image_classifier = ResNet18()\n",
    "image_classifier.to(device)\n",
    "\n",
    "epoch_num = 50\n",
    "learning_rate = 1e-6\n",
    "total_step = epoch_num * len(train_loader)\n",
    "\n",
    "optimizer = AdamW(image_classifier.parameters(), lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_step, num_training_steps=total_step)\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK4YR7cafcve",
    "outputId": "8951c4df-d8b0-453b-b164-92f36c3d9570",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(epoch_num):\n",
    "  running_loss = 0\n",
    "  for i, data in enumerate(train_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.float()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # print(inputs.shape)\n",
    "    outputs = image_classifier(inputs)\n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    running_loss += loss.item()\n",
    "  print('epoch: %d  loss: %.3f' % (epoch+1, running_loss / 35))\n",
    "  running_loss = 0"
   ],
   "metadata": {
    "id": "_BlPGK-5oJN6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "05edc796-9bb6-492a-a236-bb018bee1272",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1  loss: 1.042\n",
      "epoch: 2  loss: 1.036\n",
      "epoch: 3  loss: 1.023\n",
      "epoch: 4  loss: 1.006\n",
      "epoch: 5  loss: 0.985\n",
      "epoch: 6  loss: 0.963\n",
      "epoch: 7  loss: 0.944\n",
      "epoch: 8  loss: 0.927\n",
      "epoch: 9  loss: 0.915\n",
      "epoch: 10  loss: 0.903\n",
      "epoch: 11  loss: 0.894\n",
      "epoch: 12  loss: 0.886\n",
      "epoch: 13  loss: 0.880\n",
      "epoch: 14  loss: 0.876\n",
      "epoch: 15  loss: 0.869\n",
      "epoch: 16  loss: 0.867\n",
      "epoch: 17  loss: 0.863\n",
      "epoch: 18  loss: 0.860\n",
      "epoch: 19  loss: 0.855\n",
      "epoch: 20  loss: 0.852\n",
      "epoch: 21  loss: 0.849\n",
      "epoch: 22  loss: 0.847\n",
      "epoch: 23  loss: 0.845\n",
      "epoch: 24  loss: 0.842\n",
      "epoch: 25  loss: 0.840\n",
      "epoch: 26  loss: 0.839\n",
      "epoch: 27  loss: 0.836\n",
      "epoch: 28  loss: 0.835\n",
      "epoch: 29  loss: 0.833\n",
      "epoch: 30  loss: 0.831\n",
      "epoch: 31  loss: 0.828\n",
      "epoch: 32  loss: 0.827\n",
      "epoch: 33  loss: 0.825\n",
      "epoch: 34  loss: 0.823\n",
      "epoch: 35  loss: 0.824\n",
      "epoch: 36  loss: 0.821\n",
      "epoch: 37  loss: 0.819\n",
      "epoch: 38  loss: 0.821\n",
      "epoch: 39  loss: 0.817\n",
      "epoch: 40  loss: 0.818\n",
      "epoch: 41  loss: 0.816\n",
      "epoch: 42  loss: 0.816\n",
      "epoch: 43  loss: 0.816\n",
      "epoch: 44  loss: 0.815\n",
      "epoch: 45  loss: 0.813\n",
      "epoch: 46  loss: 0.814\n",
      "epoch: 47  loss: 0.811\n",
      "epoch: 48  loss: 0.810\n",
      "epoch: 49  loss: 0.813\n",
      "epoch: 50  loss: 0.812\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在验证集上测试参数训练的效果："
   ],
   "metadata": {
    "id": "KDqDh2mO-wwI",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "correct_num = 0\n",
    "total_num = 0\n",
    "with torch.no_grad():\n",
    "  for data in valid_loader:\n",
    "    inputs, answers = data\n",
    "    inputs = inputs.float()\n",
    "    inputs = inputs.to(device)\n",
    "    answers = answers.to(device)\n",
    "    outputs = image_classifier(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for i in range(len(predicted.tolist())):\n",
    "      total_num += answers.size(0)\n",
    "      correct_num += (predicted == answers).sum().item()\n",
    "\n",
    "print('Training Accuracy: %.3f%%' % (100 * correct_num / total_num))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VV4gl7Z6xhH_",
    "outputId": "66b2cfe9-a2d1-4291-b059-741816065568",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 60.200%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "图像分类器在图像数据集上的正确率能够达到50%左右,最高能够到达60%"
   ],
   "metadata": {
    "id": "unw4pUUQOkYo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文本分类器\n",
    "\n",
    "使用预训练模型bert-base-chinese"
   ],
   "metadata": {
    "id": "4ttYUcLSO5ap",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint = 'bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "bert_model = AutoModel.from_pretrained(checkpoint)\n",
    "# bert_model.to(device)"
   ],
   "metadata": {
    "id": "Nfb9qo0g_-B2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a48a2453-945f-4226-fc07-b0081a0015ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建文本分类器模型，在Bert的基础上增添一个线性层"
   ],
   "metadata": {
    "id": "62yOMFSd093F",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextClassifier, self).__init__()\n",
    "    self.model = bert_model\n",
    "    self.model = self.model.to(device)\n",
    "    self.dropout = nn.Dropout(0)\n",
    "    # self.model.to(device)\n",
    "    self.fc = nn.Linear(768, 3)\n",
    "  \n",
    "  def forward(self, x, attn_mask=None):\n",
    "    x = x.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    output = self.model(x, attention_mask=attn_mask)\n",
    "    # output = output.to(device)\n",
    "    output = output[1]\n",
    "    output = torch.flatten(output, 1)\n",
    "    output = self.fc(output)\n",
    "    return output"
   ],
   "metadata": {
    "id": "c1k9lOAW05sI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "验证Bert在文本分类上的性能\n",
    "\n",
    "对输入数据进行tokenize，统一长度，生成注意力分数"
   ],
   "metadata": {
    "id": "Jd8wt-fT32FO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text_train = []\n",
    "text_valid = []\n",
    "\n",
    "for data in train_set:\n",
    "  tokenized_text = tokenizer(data['text'], max_length=128, padding='max_length', truncation=True)\n",
    "  # tokenized_text['input_ids'] = torch.from_numpy(np.array(tokenized_text['input_ids']))\n",
    "  tokenized_text['label'] = data['label']\n",
    "  text_train.append(tokenized_text)\n",
    "\n",
    "for data in valid_set:\n",
    "  tokenized_text = tokenizer(data['text'], max_length=128, padding='max_length', truncation=True)\n",
    "  tokenized_text['label'] = data['label']\n",
    "  text_valid.append(tokenized_text)"
   ],
   "metadata": {
    "id": "g-a0XqgP3Uyr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "重载Dataset类，便于生成Dataloader"
   ],
   "metadata": {
    "id": "Zt9gpoglLO7P",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    super(TextDataset, self).__init__()\n",
    "    self.data = data\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    input_ids = self.data[idx]['input_ids']\n",
    "    attn_mask = self.data[idx]['attention_mask']\n",
    "    label = self.data[idx]['label']\n",
    "    return input_ids, attn_mask, label\n",
    "\n",
    "train_loader = DataLoader(TextDataset(text_train), batch_size=25, shuffle=True)\n",
    "valid_loader = DataLoader(TextDataset(text_valid), batch_size=25)"
   ],
   "metadata": {
    "id": "vo1GV-CKLHae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.to(device)\n",
    "# classifier.model.to(device)\n",
    "\n",
    "epoch_num = 20\n",
    "learning_rate = 1e-5\n",
    "total_step = epoch_num * len(train_loader)\n",
    "\n",
    "optimizer = AdamW(text_classifier.parameters(), lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_step, num_training_steps=total_step)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lPWfOdsLJRM",
    "outputId": "92736dc5-f54e-455b-b426-3def9dad2123",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# classifier.train()\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "  running_loss = 0\n",
    "  for i, data in enumerate(train_loader):\n",
    "    input_ids, attn_mask, labels = data\n",
    "    input_ids = torch.tensor([item.numpy() for item in input_ids])\n",
    "    attn_mask = torch.tensor([item.numpy() for item in attn_mask])\n",
    "    input_ids = input_ids.T\n",
    "    attn_mask = attn_mask.T\n",
    "    # labels = torch.tensor([item.numpy() for item in labels])\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # print(input_ids.shape)\n",
    "    # print(attn_mask.shape)\n",
    "\n",
    "    outputs = text_classifier(input_ids, attn_mask)\n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "  print('epoch: %d  loss: %.3f' % (epoch+1, running_loss/140))\n",
    "  running_loss = 0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQGBwjmFOQ15",
    "outputId": "a6e32cb8-0552-49e2-b819-2c783866f1cb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1  loss: 0.995\n",
      "epoch: 2  loss: 0.853\n",
      "epoch: 3  loss: 0.764\n",
      "epoch: 4  loss: 0.559\n",
      "epoch: 5  loss: 0.306\n",
      "epoch: 6  loss: 0.165\n",
      "epoch: 7  loss: 0.112\n",
      "epoch: 8  loss: 0.080\n",
      "epoch: 9  loss: 0.065\n",
      "epoch: 10  loss: 0.059\n",
      "epoch: 11  loss: 0.051\n",
      "epoch: 12  loss: 0.046\n",
      "epoch: 13  loss: 0.043\n",
      "epoch: 14  loss: 0.040\n",
      "epoch: 15  loss: 0.038\n",
      "epoch: 16  loss: 0.037\n",
      "epoch: 17  loss: 0.034\n",
      "epoch: 18  loss: 0.034\n",
      "epoch: 19  loss: 0.032\n",
      "epoch: 20  loss: 0.031\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "correct_num = 0\n",
    "total_num = 0\n",
    "with torch.no_grad():\n",
    "  for data in valid_loader:\n",
    "    input_ids, attn_mask, labels = data\n",
    "    input_ids = torch.tensor([item.numpy() for item in input_ids])\n",
    "    input_ids = input_ids.T\n",
    "    attn_mask = torch.tensor([item.numpy() for item in attn_mask])\n",
    "    attn_mask = attn_mask.T\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = text_classifier(input_ids, attn_mask)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for i in range(len(predicted.tolist())):\n",
    "      total_num += labels.size(0)\n",
    "      correct_num += (predicted == labels).sum().item()\n",
    "\n",
    "print('Training Accuracy: %.3f%%' % (100 * correct_num / total_num))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR3QFeUYd6mR",
    "outputId": "ad3cf93f-92ff-4721-d52e-30cf51effafd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 62.600%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "基于bert的文本分类器在数据集上的正确率能够到达60%"
   ],
   "metadata": {
    "id": "UVKzLvpQ6ECf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 融合模型\n",
    "\n",
    "定义完整的Dataset，输入时向文本分类器提供文本，图片分类器提供图片"
   ],
   "metadata": {
    "id": "uoflU7xK7j4G",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    super(MultimodalDataset, self).__init__()\n",
    "    self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    guid = self.data[idx]['guid']\n",
    "    input_ids = torch.tensor(self.data[idx]['input_ids'])\n",
    "    attn_mask = torch.tensor(self.data[idx]['attn_mask'])\n",
    "    image = torch.tensor(self.data[idx]['image'])\n",
    "    label = self.data[idx].get('label')\n",
    "    if label is None:\n",
    "      label = -100\n",
    "    label = torch.tensor(label)\n",
    "    return guid, input_ids, attn_mask, image, label"
   ],
   "metadata": {
    "id": "9xsB3e_1eMrd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def dataset_process(dataset):\n",
    "  for data in dataset:\n",
    "    tokenized_text = tokenizer(data['text'], max_length=128, padding='max_length', truncation=True)\n",
    "    data['input_ids'] = tokenized_text['input_ids']\n",
    "    data['attn_mask'] = tokenized_text['attention_mask']"
   ],
   "metadata": {
    "id": "ZqNWx3_QIPCp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_process(train_set)\n",
    "dataset_process(valid_set)\n",
    "dataset_process(test_set)"
   ],
   "metadata": {
    "id": "L2sbdm73RxuP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(MultimodalDataset(train_set), batch_size=25, shuffle=True)\n",
    "valid_loader = DataLoader(MultimodalDataset(valid_set), batch_size=25)\n",
    "test_loader = DataLoader(MultimodalDataset(test_set), batch_size=25)"
   ],
   "metadata": {
    "id": "YnuYqyjSzLO_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "构建完整的融合模型类，文本分类器的输出结果与图像分类器的输出结果拼接，经过线性层分类后输出\n",
    "\n",
    "两个分类器的输出shape均为为(batch_size, output_features)\n",
    "\n",
    "对拼接后的特征向量，先进入一个线性层，使模型学习两个特征向量之间的关系\n",
    "\n",
    "最后进入分类层，输出结果"
   ],
   "metadata": {
    "id": "aD14pVB3V2_a",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "  def __init__(self, image_classifier, text_classifier, output_features, image_weight=0.5, text_weight=0.5):\n",
    "    super(MultimodalModel, self).__init__()\n",
    "    self.image_classifier = image_classifier\n",
    "    self.text_classifier = text_classifier\n",
    "    # 将最后的全连接层删除\n",
    "    self.image_classifier.fc = nn.Sequential()  # (batch_num, 512)\n",
    "    self.text_classifier.fc = nn.Sequential()    # (batch_num, 768)\n",
    "    # 文本特征向量和图片特征向量的权重, 默认均为0.5\n",
    "    self.image_weight = image_weight\n",
    "    self.text_weight = text_weight\n",
    "    self.fc1 = nn.Linear((512+768), output_features)\n",
    "    self.fc2 = nn.Linear(output_features, 3)\n",
    "\n",
    "  def forward(self, input_ids, attn_mask, image):\n",
    "    image_output = self.image_classifier(image)\n",
    "    text_output = self.text_classifier(input_ids, attn_mask)\n",
    "    output = torch.cat([image_output, text_output], dim=-1)\n",
    "    output = self.fc1(output)\n",
    "    output = self.fc2(output)\n",
    "    return output"
   ],
   "metadata": {
    "id": "4JXGc8VqV20Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "实例化时使用先前训练完成的模型，在各自数据集上的分类效果较好，提取出的特征向量表现相较于初始化的模型也更优"
   ],
   "metadata": {
    "id": "D5762Eb4RSJf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "multimodal_model = MultimodalModel(image_classifier=image_classifier, text_classifier=text_classifier, output_features=100, image_weight=0.5, text_weight=0.5)\n",
    "multimodal_model.to(device)\n",
    "\n",
    "epoch_num = 10\n",
    "learning_rate = 1e-5\n",
    "total_step = epoch_num * len(train_loader)\n",
    "\n",
    "optimizer = AdamW(multimodal_model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_step, num_training_steps=total_step)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLn_6pIJjDvw",
    "outputId": "ef66bc8b-3a91-47bb-c3cd-a4604d5bf208",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(epoch_num):\n",
    "  running_loss = 0\n",
    "  for i, data in enumerate(train_loader):\n",
    "    _, input_ids, attn_mask, image, label = data\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    image = image.to(device)\n",
    "    image = image.float()\n",
    "    label = label.to(device)\n",
    "\n",
    "    outputs = multimodal_model(input_ids=input_ids, attn_mask=attn_mask, image=image)\n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs, label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "  print('epoch: %d  loss: %.3f' % (epoch+1, running_loss/140))\n",
    "  running_loss = 0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAW1LqY8_mMJ",
    "outputId": "87736f33-b45c-4566-f7d4-a4381400b074",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1  loss: 0.575\n",
      "epoch: 2  loss: 0.153\n",
      "epoch: 3  loss: 0.097\n",
      "epoch: 4  loss: 0.083\n",
      "epoch: 5  loss: 0.067\n",
      "epoch: 6  loss: 0.044\n",
      "epoch: 7  loss: 0.040\n",
      "epoch: 8  loss: 0.033\n",
      "epoch: 9  loss: 0.030\n",
      "epoch: 10  loss: 0.028\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "correct_num = 0\n",
    "total_num = 0\n",
    "with torch.no_grad():\n",
    "  for data in valid_loader:\n",
    "    _, input_ids, attn_mask, image, label = data\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    image = image.to(device)\n",
    "    image = image.float()\n",
    "    label = label.to(device)\n",
    "    \n",
    "    outputs = multimodal_model(input_ids=input_ids, attn_mask=attn_mask, image=image)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    for i in range(len(predicted.tolist())):\n",
    "      total_num += label.size(0)\n",
    "      correct_num += (predicted == label).sum().item()\n",
    "\n",
    "print('Training Accuracy: %.3f%%' % (100 * correct_num / total_num))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLRkTUpaHWsH",
    "outputId": "e9a0df80-df34-4ad0-f2ba-8b53e2fdbf94",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 62.600%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_dict = {}\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    guid, input_ids, attn_mask, image, label = data\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "    image = image.to(device)\n",
    "    image = image.float()\n",
    "    label = label.to(device)\n",
    "    \n",
    "    outputs = multimodal_model(input_ids=input_ids, attn_mask=attn_mask, image=image)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = predicted.tolist()\n",
    "    for i in range(len(predicted)):\n",
    "      id = guid[i]\n",
    "      test_dict[id] = predicted[i]"
   ],
   "metadata": {
    "id": "Qj0B316-KFSV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "将分类结果重新写入文件"
   ],
   "metadata": {
    "id": "Pch0kcr-1E4X",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open('./test_without_label.txt', 'r') as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "f1 = open('./test.txt', 'w')\n",
    "f1.write(lines[0])\n",
    "\n",
    "for line in lines[1:]:\n",
    "  # print(line)\n",
    "  guid = line.split(',')[0]\n",
    "  f1.write(guid)\n",
    "  f1.write(',')\n",
    "  label = test_dict[guid]\n",
    "  if label == 0:\n",
    "    f1.write('positive\\n')\n",
    "  elif label == 1:\n",
    "    f1.write('neutral\\n')\n",
    "  else:\n",
    "    f1.write('negative\\n')"
   ],
   "metadata": {
    "id": "chnQVyb-cknf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 33,
   "outputs": []
  }
 ]
}